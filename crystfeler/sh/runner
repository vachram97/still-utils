#!/bin/bash

HELP_STRING="\
Usage: runner input.yaml

See full documentation here: https://github.com/marinegor/still-utils
Or email marin@phystech.edu"

# reading input
if [ "$1" == "-h" ]; then
  echo "Usage: $(basename "$0") ${HELP_STRING}"
  exit 0
fi


# with courtesy to amazing @jasperes: https://github.com/jasperes/bash-yaml
#--------------------------------------------------------------------------
parse_yaml() {
    local yaml_file=$1
    local prefix=$2
    local s
    local w
    local fs

    s='[[:space:]]*'
    w='[a-zA-Z0-9_.-]*'
    fs="$(echo @|tr @ '\034')"

    (
        sed -e '/- [^\â€œ]'"[^\']"'.*: /s|\([ ]*\)- \([[:space:]]*\)|\1-\'$'\n''  \1\2|g' |

        sed -ne '/^--/s|--||g; s|\"|\\\"|g; s/[[:space:]]*$//g;' \
            -e "/#.*[\"\']/!s| #.*||g; /^#/s|#.*||g;" \
            -e "s|^\($s\)\($w\)$s:$s\"\(.*\)\"$s\$|\1$fs\2$fs\3|p" \
            -e "s|^\($s\)\($w\)${s}[:-]$s\(.*\)$s\$|\1$fs\2$fs\3|p" |

        awk -F"$fs" '{
            indent = length($1)/2;
            if (length($2) == 0) { conj[indent]="+";} else {conj[indent]="";}
            vname[indent] = $2;
            for (i in vname) {if (i > indent) {delete vname[i]}}
                if (length($3) > 0) {
                    vn=""; for (i=0; i<indent; i++) {vn=(vn)(vname[i])("_")}
                    printf("%s%s%s%s=(\"%s\")\n", "'"$prefix"'",vn, $2, conj[indent-1],$3);
                }
            }' |

        sed -e 's/_=/+=/g' |

        awk 'BEGIN {
                FS="=";
                OFS="="
            }
            /(-|\.).*=/ {
                gsub("-|\\.", "_", $1)
            }
            { print }'
    ) < "$yaml_file"
}

create_variables() {
    local yaml_file="$1"
    local prefix="$2"
    eval "$(parse_yaml "$yaml_file" "$prefix")"
}
#--------------------------------------------------------------------------

# debug flag
DEBUG="FALSE"

# merging offline by default
MERGE_OFFLINE="TRUE"

# SLURM parameters
SLURM_HEADER_FILE="slurm.header"
SLURM_SETUP="module load apps/crystfel-0.9.0"

# Essential folders
time=$(date "+%Y_%m_%d_%H_%M_%S")
LOGDIR="${PWD}/logs"; mkdir "${LOGDIR}" &> /dev/null
STREAMDIR="${PWD}/streams"; mkdir "${STREAMDIR}" &> /dev/null



TOP="none"
for var in "$@"; do
	if ! create_variables "$var"; then
		echo "Input file ${var} is invalid"; exit 1
	fi
done
TOP="$(echo -e "${TOP}" | tr -d '[:space:]')"

CURRENT_LOGDIR="${LOGDIR}/${PROJECT_NAME}_${time}"
mkdir -p "${CURRENT_LOGDIR}/indexer-logs" 2>/dev/null || exit 1;

for var in "$@"; do
	cp $var "$CURRENT_LOGDIR"
done
# check if path is absolute, if not -- make it
if ! [[ ${GEOM::1} == "/" ]]; then GEOM="${PWD}/${GEOM}"; fi
if ! [[ ${CELL::1} == "/" ]]; then CELL="${PWD}/${CELL}"; fi
if ! [[ ${LST::1} == "/" ]]; then LST="${PWD}/${LST}"; fi

# set default non-influencing SNR & THRESHOLD if cxi or h5

if [[ "$PEAKS" == "cxi" ]] || [[ "$PEAKS" == "h5" ]]; then
	SNR="0.0"
	THRESHOLD="1"
	HIGHRES="1.0"
fi

# Shuffling the stream
if   [[ "$SHUFFLE" == 1 ]] && [[ "$TOP" == "none" ]]; then
	shuf "$LST" > input.lst # your list must have events to enable this
elif [[ "$SHUFFLE" == 1 ]] && [[ "$TOP" != "none" ]]; then
	shuf "$LST" | head -n "$TOP" > input.lst
elif [[ "$SHUFFLE" == 0 ]] && [[ "$TOP" != "none" ]]; then
	head -n "$TOP" "$LST" > input.lst
elif [[ "$SHUFFLE" == 0 ]] && [[ "$TOP" == "none" ]]; then
	cat "$LST" > input.lst
fi
LST="input.lst";


# Start building the indexamajig command
BASEEXECSTRING="indexamajig \
-g ${GEOM} \
-p ${CELL} \
--min-snr=${SNR} \
--threshold=${THRESHOLD} \
--highres=${HIGHRES} \
--temp-dir="/tmp" \
--min-peaks=${MINPEAKS} \
${OTHERPARAMS}"

touch "streams/${PROJECT_NAME}_${time}.stream"
ln -sf "streams/${PROJECT_NAME}_${time}.stream" laststream
if [[ "${SLURM}" == "1" ]]; then
	NLINES_INPUT="$(wc -l ${LST} | awk '{print $1}')"

	# Divide initial list into separate lists for each job
	split --numeric-suffixes=1 \
		--suffix-length=4 \
		--additional-suffix=".lst" \
		--lines=${NODELINES} \
		"${LST}" "${CURRENT_LOGDIR}/list_"
	LST_NUM=$(ls ${CURRENT_LOGDIR}/list_* | wc -l)
	if [[ LST_NUM -gt 1000 ]]; then
		echo "Too many jobs in array! Change NODELINES!"
		exit 1;
	fi

	# print headers to worker script
	
	# here echo header
	current_worker=${CURRENT_LOGDIR}/worker.sh
	cat ${SLURM_HEADER_FILE} >> "${current_worker}"

	echo "#SBATCH --chdir ${CURRENT_LOGDIR}" >> "${current_worker}" >> ${current_worker}
	# echo "#SBATCH --job-name ${PROJECT_NAME:0:8}_w${worker_num}" >> ${current_worker}
	echo "#SBATCH --job-name ${PROJECT_NAME:0:8}" >> ${current_worker}
	echo "#SBATCH --output ${CURRENT_LOGDIR}/stdout_%a.log" >> ${current_worker}
	echo "#SBATCH --error ${CURRENT_LOGDIR}/stderr_%a.log" >> ${current_worker}
	echo "#SBATCH --nodes 1" >> ${current_worker}
	echo "#SBATCH --exclusive" >> ${current_worker}
	if [[ ! -z $EXCLUDE_NODES ]]; then
		echo "#SBATCH --exclude=${EXCLUDE_NODES}" >> ${current_worker}
	fi
	if [[ -z $MAX_NODES ]]; then 
		echo "#SBATCH --array=1-${LST_NUM}" >> ${current_worker}
	else
		echo "#SBATCH --array=1-${LST_NUM}%${MAX_NODES}" >> ${current_worker}
	fi
	echo >> "${current_worker}"
	echo "${SLURM_SETUP}" >> "${current_worker}"
	echo >> "${current_worker}"

	echo 'printf -v TASK_ID "%04d\n" ${SLURM_ARRAY_TASK_ID}' >> ${current_worker}
	echo "TASK_ID=\${TASK_ID%$'\n'}" >> ${current_worker}

	# writing tail line
	if [[ "${MERGE_OFFLINE}" == "FALSE" ]]; then

		current_stream=${CURRENT_LOGDIR}/stream_\${TASK_ID}.stream
	
		echo "touch ${current_stream}" >> "${current_worker}"
		echo "tail -f ${current_stream} --pid=\$\$ | pylock ${STREAMDIR}/${PROJECT_NAME}_${time}.stream & disown" >> "${current_worker}"

	fi

	# writing multiple-worker execution strings

		
	current_stream=${CURRENT_LOGDIR}/stream_\${TASK_ID}.stream
	current_lst=${CURRENT_LOGDIR}/list_\${TASK_ID}.lst

	# now writing the execution string itself
	EXECSTRING="${BASEEXECSTRING} -o \"${current_stream}\" -i \"${current_lst}\""
	EXECSTRING="${EXECSTRING} --peaks=${PEAKS} --indexing=${INDEXING}"
	EXECSTRING="${EXECSTRING} -j ${NPROC}"  # this will use the `nproc` value on host machine

	echo "$EXECSTRING &" >> "${current_worker}"

	# echo "cat ${current_stream} | pylock ${STREAMDIR}/${PROJECT_NAME}_${time}.stream.final" >> "${current_worker}"
	# targz partial streams to save some space
	# echo "tar fcz ${current_stream}.tar.gz ${current_stream} --absolute-names" >> "${current_worker}"

	# modifying node numbers and running sbatch-ing workers

	echo "wait" >> "${current_worker}"
	if [[ "${DEBUG}" == "FALSE" ]]; then
		JOB_NUMBER="$(sbatch "${current_worker}" | awk '{ print $4 }')"
	else
		JOB_NUMBER="1"
	fi

	echo "Job numbers submitted are ${JOB_NUMBER}"

	if [[ "${MERGE_OFFLINE}" == "TRUE" ]]; then

		merger=${CURRENT_LOGDIR}/merger.sh
		cat ${SLURM_HEADER_FILE} >> "${merger}"
		echo "#SBATCH --chdir ${CURRENT_LOGDIR}" >> "${merger}" >> ${merger}
		echo "#SBATCH --job-name ${PROJECT_NAME:0:8}" >> ${merger}
		echo "#SBATCH --output ${CURRENT_LOGDIR}/stdout_merge.log" >> ${merger}
		echo "#SBATCH --error ${CURRENT_LOGDIR}/stderr_merge.log" >> ${merger}
		echo "#SBATCH --ntasks=1" >> ${merger}
		echo "#SBATCH --cpus-per-task=1" >> ${merger}

		dep_string="#SBATCH --dependency=afterany"
		dep_string="${dep_string}:${JOB_NUMBER}"

		echo "${dep_string}" >> ${merger}
		echo >> "${merger}"

		echo "for strm in \"${CURRENT_LOGDIR}/stream_*.stream\"; do
			echo \"Found \${strm}\"
			cat \${strm} >> ${STREAMDIR}/${PROJECT_NAME}_${time}.stream
		done" >> "${merger}"
		
		if [[ "${DEBUG}" == "FALSE" ]]; then
			sleep 5
			sbatch "${merger}"
		fi
	fi

	echo "tail -f ${CURRENT_LOGDIR}/stderr_*.log"

else
	# linking the laststream
	EXECSTRING="${BASEEXECSTRING} -o ${STREAMDIR}/${PROJECT_NAME}_${time}.stream -i ${LST}"
	EXECSTRING="${EXECSTRING} --peaks=${PEAKS} --indexing=${INDEXING}"
	EXECSTRING="${EXECSTRING} -j ${NPROC}"
	echo "$EXECSTRING"  
	eval "${EXECSTRING}" |& tee "${LOGDIR}/log.indexamajig_${time}"
fi
